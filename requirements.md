# SoundMem需求文档

## 功能总述
本项目会记录用户的录音，使用本地的ASR工具转成文字，自动进行分段等操作，构建向量数据库。用户提问跟录音中有关的问题，系统采用RAG的方式，根据录音中的内容进行回答。

## 技术架构

### 1. 录音模块
- **音频采集**: 使用 `sounddevice` 或 `pyaudio` 进行实时音频流采集
- **音频格式**: 16kHz采样率，16bit，单声道（满足ASR模型输入要求）
- **缓冲机制**: 采用滑动窗口，每3-5秒为一个处理单元，避免内存溢出
- **音频存储**: 可选保存原始录音文件（WAV格式），便于后续回溯

### 2. ASR模块（语音转文字）
- **模型选择**: FunASR的 `paraformer-zh` 模型
  - 支持中英文混合识别
  - CPU推理友好，适合端侧部署
  - 支持实时流式识别
  - 模型大小约200MB，加载速度快
- **备选方案**: `SenseVoiceSmall` (更轻量，约50MB)
- **VAD集成**: 使用FunASR的 `fsmn-vad` 进行语音活动检测，自动过滤静音片段
- **标点恢复**: 使用 `ct-punc` 模型自动添加标点符号

### 3. 文本处理模块
- **分段策略**:
  - 基于VAD的自然停顿点分段
  - 基于标点符号的语义分段
  - 长文本按300-500字符切分，保持语义完整性
  - 添加时间戳标记每个片段的录制时间
- **文本清洗**: 去除重复、修正常见ASR错误
- **元数据**: 记录每段文本的时间范围、说话人标识（可选）

### 4. 向量数据库
- **选型**: **Chroma DB**（推荐）
  - 轻量级，易于集成，支持本地部署
  - 内置向量索引（HNSW算法），检索速度快
  - 支持实时增量更新
  - 支持元数据过滤（如时间范围查询）
  - Python原生支持，无需额外服务
- **备选方案**:
  - **FAISS** (Facebook AI): 性能最优，但需要额外的元数据管理
  - **Qdrant**: 功能强大，但需要独立服务，部署复杂
- **向量模型**: 
  - 主选: `bge-small-zh-v1.5` (中文优化，模型小约100MB)
  - 备选: `text2vec-base-chinese` 或 `m3e-small`
- **索引策略**: 
  - 每处理完一段文本立即插入向量库
  - 异步批量更新索引，避免阻塞录音流程

### 5. RAG检索模块
- **检索策略**:
  - 混合检索：向量相似度 + 关键词匹配（BM25）
  - Top-K检索（K=3-5），可根据相似度阈值动态调整
  - 时间加权：优先检索最近的录音内容
- **上下文构建**:
  - 检索到的片段按时间顺序排列
  - 添加时间戳信息，便于用户定位
  - 控制总token数在模型上下文窗口内（如4K tokens）

### 6. 大模型对话模块
- **API兼容**: 支持OpenAI格式API（OpenAI、DeepSeek、Kimi、通义千问等）
- **配置管理**: 
  - API Key、Base URL、模型名称可配置
  - 支持流式输出（SSE）
  - 可设置temperature、max_tokens等参数
- **Prompt工程**:
  - 系统提示词：明确角色为"录音内容助手"
  - 上下文注入：将检索到的录音片段作为参考资料
  - 引用标注：回答时标注信息来源的时间点

### 7. 前端界面
- **技术栈**: 
  - **Gradio**（推荐）: 快速搭建，适合原型和本地部署
  - 备选: Streamlit 或 Flask + Vue.js
- **界面布局**:
  - 左侧：录音控制区
    - 大号"开始录音"/"停止录音"按钮（带状态指示灯）
    - 实时转写文本显示区（滚动显示最新内容）
    - 录音时长计时器
    - 已处理文本片段数量统计
  - 右侧：对话区
    - 聊天界面（类ChatGPT风格）
    - 输入框 + 发送按钮
    - 历史对话记录
    - 回答中的时间戳可点击定位到原始录音
  - 顶部：配置区（可折叠）
    - API配置（Key、URL、模型）
    - 向量库管理（清空、导出）
    - 录音设备选择

## 界面介绍
界面中应该至少包括以下内容：
- 明显的"开始记录"和"停止记录"的按钮，一旦开始记录就会开始录音，转文字，加入数据库等操作。
- 一个和大模型的对话框，对话框内可以跟大模型对话，提问跟录音中内容有关的问题。

## 数据流程

```
录音 → 音频流 → VAD检测 → ASR转文字 → 文本分段 → 向量化 → 存入Chroma
                                                              ↓
用户提问 → 向量化 → Chroma检索 → 构建Prompt → 调用LLM API → 返回答案
```

## 环境配置

### Conda环境
```bash
conda create -n soundmem python=3.10
conda activate soundmem
```

### 核心依赖
- **音频处理**: sounddevice, numpy, scipy
- **ASR**: funasr, modelscope
- **向量数据库**: chromadb
- **向量模型**: sentence-transformers
- **LLM调用**: openai (官方SDK兼容多家API)
- **前端**: gradio
- **工具库**: python-dotenv (配置管理), loguru (日志)

## 性能优化

### 实时性保障
1. **多线程架构**:
   - 线程1: 音频采集
   - 线程2: ASR转写
   - 线程3: 向量化与入库
   - 线程4: Web服务
2. **队列缓冲**: 使用 `queue.Queue` 在线程间传递数据
3. **模型预加载**: 启动时加载所有模型到内存
4. **批处理**: 向量化支持批量处理，提高吞吐量

### 内存管理
- 限制音频缓冲区大小（如最多保留最近1小时）
- 向量库定期压缩（Chroma自动管理）
- 长时间录音可选择分session存储

## 扩展功能（可选）

1. **多会话管理**: 支持创建多个录音会话，独立的向量库
2. **说话人分离**: 集成pyannote.audio进行多人对话场景
3. **关键词提取**: 自动生成录音摘要和关键词
4. **导出功能**: 导出转写文本、对话记录
5. **离线模式**: 集成本地小模型（如Qwen-1.8B）作为备选

## 注意事项

1. **API兼容性**: 本项目中的大模型使用调用API的方式，应当可以支持所有OpenAI格式的API
2. **ASR要求**: ASR模型最好使用FunASR中的模型，确保本项目能在没有GPU的端侧电脑上可以本地部署，并同时支持中英文
3. **实时性**: 向量数据库应该可以实时更新，可以边录音边问问题，给出的回答也应具有较强的实时性
4. **环境隔离**: 创建一个新的名叫soundmem的conda环境来安装需要的库并运行程序
5. **隐私保护**: 所有数据本地存储，不上传云端（除API调用）
6. **错误处理**: ASR识别错误、网络异常等需要优雅降级
7. **跨平台**: 确保Windows/Mac/Linux兼容性
8. **资源占用**: CPU使用率控制在50%以内，内存<2GB
9. **用户体验**: 
   - 首次启动自动下载模型（显示进度）
   - 提供配置向导
   - 异常情况给出明确提示
